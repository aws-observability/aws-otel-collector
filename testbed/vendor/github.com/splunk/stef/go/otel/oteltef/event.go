// Code generated by stefc. DO NOT EDIT.
package oteltef

import (
	"bytes"
	"fmt"
	"math/rand/v2"
	"strings"
	"unsafe"

	"github.com/splunk/stef/go/pkg"
	"github.com/splunk/stef/go/pkg/encoders"
	"github.com/splunk/stef/go/pkg/schema"
)

var _ = strings.Compare
var _ = encoders.StringEncoder{}
var _ = schema.WireSchema{}
var _ = bytes.NewBuffer

type Event struct {
	name                   string
	timeUnixNano           uint64
	attributes             Attributes
	droppedAttributesCount uint64

	// modifiedFields keeps track of which fields are modified.
	modifiedFields modifiedFields
}

const EventStructName = "Event"

// Bitmasks for "modified" flags for each field.
const (
	fieldModifiedEventName = uint64(1 << iota)
	fieldModifiedEventTimeUnixNano
	fieldModifiedEventAttributes
	fieldModifiedEventDroppedAttributesCount
)

// Init must be called once, before the Event is used.
func (s *Event) Init() {
	s.init(nil, 0)
}

func NewEvent() *Event {
	var s Event
	s.init(nil, 0)
	return &s
}

func (s *Event) init(parentModifiedFields *modifiedFields, parentModifiedBit uint64) {
	s.modifiedFields.parent = parentModifiedFields
	s.modifiedFields.parentBit = parentModifiedBit

	s.attributes.init(&s.modifiedFields, fieldModifiedEventAttributes)
}

func (s *Event) initAlloc(parentModifiedFields *modifiedFields, parentModifiedBit uint64, allocators *Allocators) {
	s.modifiedFields.parent = parentModifiedFields
	s.modifiedFields.parentBit = parentModifiedBit

	s.attributes.initAlloc(&s.modifiedFields, fieldModifiedEventAttributes, allocators)
}

// reset the struct to its initial state, as if init() was just called.
// Will not reset internal fields such as parentModifiedFields.
func (s *Event) reset() {
	s.name = ""
	s.timeUnixNano = 0
	s.attributes.reset()
	s.droppedAttributesCount = 0
}

// fixParent sets the parentModifiedFields pointer to the supplied value.
// This is used when the parent is moved in memory for example because the parent
// an array element and the array was expanded.
func (s *Event) fixParent(parentModifiedFields *modifiedFields) {
	s.modifiedFields.parent = parentModifiedFields
	s.attributes.fixParent(&s.modifiedFields)
}

// Freeze the struct. Any attempt to modify it after this will panic.
// This marks the struct as eligible for safely sharing by pointer without cloning,
// which can improve encoding performance.
func (s *Event) Freeze() {
	s.modifiedFields.freeze()
}

func (s *Event) isFrozen() bool {
	return s.modifiedFields.isFrozen()
}

func (s *Event) Name() string {
	return s.name
}

// SetName sets the value of Name field.
func (s *Event) SetName(v string) {
	if s.name != v {
		s.name = v
		s.modifiedFields.markModified(fieldModifiedEventName)
	}
}

func (s *Event) markNameModified() {
	s.modifiedFields.markModified(fieldModifiedEventName)
}

// IsNameModified returns true the value of Name field was modified since
// Event was created, encoded or decoded. If the field is modified
// it will be encoded by the next Write() operation. If the field is decoded by the
// next Read() operation the modified flag will be set.
func (s *Event) IsNameModified() bool {
	return s.modifiedFields.mask&fieldModifiedEventName != 0
}

func (s *Event) TimeUnixNano() uint64 {
	return s.timeUnixNano
}

// SetTimeUnixNano sets the value of TimeUnixNano field.
func (s *Event) SetTimeUnixNano(v uint64) {
	if s.timeUnixNano != v {
		s.timeUnixNano = v
		s.modifiedFields.markModified(fieldModifiedEventTimeUnixNano)
	}
}

func (s *Event) markTimeUnixNanoModified() {
	s.modifiedFields.markModified(fieldModifiedEventTimeUnixNano)
}

// IsTimeUnixNanoModified returns true the value of TimeUnixNano field was modified since
// Event was created, encoded or decoded. If the field is modified
// it will be encoded by the next Write() operation. If the field is decoded by the
// next Read() operation the modified flag will be set.
func (s *Event) IsTimeUnixNanoModified() bool {
	return s.modifiedFields.mask&fieldModifiedEventTimeUnixNano != 0
}

func (s *Event) Attributes() *Attributes {
	return &s.attributes
}

func (s *Event) markAttributesModified() {
	s.modifiedFields.markModified(fieldModifiedEventAttributes)
}

// IsAttributesModified returns true the value of Attributes field was modified since
// Event was created, encoded or decoded. If the field is modified
// it will be encoded by the next Write() operation. If the field is decoded by the
// next Read() operation the modified flag will be set.
func (s *Event) IsAttributesModified() bool {
	return s.modifiedFields.mask&fieldModifiedEventAttributes != 0
}

func (s *Event) DroppedAttributesCount() uint64 {
	return s.droppedAttributesCount
}

// SetDroppedAttributesCount sets the value of DroppedAttributesCount field.
func (s *Event) SetDroppedAttributesCount(v uint64) {
	if s.droppedAttributesCount != v {
		s.droppedAttributesCount = v
		s.modifiedFields.markModified(fieldModifiedEventDroppedAttributesCount)
	}
}

func (s *Event) markDroppedAttributesCountModified() {
	s.modifiedFields.markModified(fieldModifiedEventDroppedAttributesCount)
}

// IsDroppedAttributesCountModified returns true the value of DroppedAttributesCount field was modified since
// Event was created, encoded or decoded. If the field is modified
// it will be encoded by the next Write() operation. If the field is decoded by the
// next Read() operation the modified flag will be set.
func (s *Event) IsDroppedAttributesCountModified() bool {
	return s.modifiedFields.mask&fieldModifiedEventDroppedAttributesCount != 0
}

func (s *Event) setModifiedRecursively() {
	s.attributes.setModifiedRecursively()
	s.modifiedFields.mask =
		fieldModifiedEventName |
			fieldModifiedEventTimeUnixNano |
			fieldModifiedEventAttributes |
			fieldModifiedEventDroppedAttributesCount | 0
}

func (s *Event) setUnmodifiedRecursively() {
	if s.IsAttributesModified() {
		s.attributes.setUnmodifiedRecursively()
	}
	s.modifiedFields.mask = 0
}

// computeDiff compares s and val and returns true if they differ.
// All fields that are different in s will be marked as modified.
func (s *Event) computeDiff(val *Event) (ret bool) {
	// Compare Name field.
	if s.name != val.name {
		s.modifiedFields.setModified(fieldModifiedEventName)
		ret = true
	}
	// Compare TimeUnixNano field.
	if s.timeUnixNano != val.timeUnixNano {
		s.modifiedFields.setModified(fieldModifiedEventTimeUnixNano)
		ret = true
	}
	// Compare Attributes field.
	if s.attributes.computeDiff(&val.attributes) {
		s.modifiedFields.setModified(fieldModifiedEventAttributes)
		ret = true
	}
	// Compare DroppedAttributesCount field.
	if s.droppedAttributesCount != val.droppedAttributesCount {
		s.modifiedFields.setModified(fieldModifiedEventDroppedAttributesCount)
		ret = true
	}
	return ret
}

// canBeShared returns true if s is safe to share by pointer without cloning (for example if s is frozen).
func (s *Event) canBeShared() bool {
	return false
}

// CloneShared returns a clone of s. It may return s if it is safe to share without cloning
// (for example if s is frozen).
func (s *Event) CloneShared(allocators *Allocators) Event {
	return s.Clone(allocators)
}

func (s *Event) Clone(allocators *Allocators) Event {
	c := Event{
		name:                   s.name,
		timeUnixNano:           s.timeUnixNano,
		droppedAttributesCount: s.droppedAttributesCount,
	}
	copyToNewAttributes(&c.attributes, &s.attributes, allocators)
	return c
}

// ByteSize returns approximate memory usage in bytes. Used to calculate
// memory used by dictionaries.
func (s *Event) byteSize() uint {
	return uint(unsafe.Sizeof(*s)) +
		s.attributes.byteSize() + 0
}

// Copy from src to dst, overwriting existing data in dst.
func copyEvent(dst *Event, src *Event) {
	dst.SetName(src.name)
	dst.SetTimeUnixNano(src.timeUnixNano)
	copyAttributes(&dst.attributes, &src.attributes)
	dst.SetDroppedAttributesCount(src.droppedAttributesCount)
}

// Copy from src to dst. dst is assumed to be just inited.
func copyToNewEvent(dst *Event, src *Event, allocators *Allocators) {
	dst.SetName(src.name)
	dst.SetTimeUnixNano(src.timeUnixNano)
	copyToNewAttributes(&dst.attributes, &src.attributes, allocators)
	dst.SetDroppedAttributesCount(src.droppedAttributesCount)
}

// CopyFrom() performs a deep copy from src.
func (s *Event) CopyFrom(src *Event) {
	copyEvent(s, src)
}

// mutateRandom mutates fields in a random, deterministic manner using
// random parameter as a deterministic generator. Only fields that exist
// in the schem are mutated, allowing to generate data for specified schema.
func (s *Event) mutateRandom(random *rand.Rand, schem *schema.Schema) {
	// Get the field count for this struct from the schema. If the schema specifies
	// fewer field count than the one we have in this code then we will not mutate
	// fields that are not in the schema.
	fieldCount, err := schem.FieldCount("Event")
	if err != nil {
		panic(fmt.Sprintf("cannot get field count for %s: %v", "Event", err))
	}

	const randRange = max(4, 2) // At least 2 to ensure we don't recurse infinitely if there is only 1 field.

	if fieldCount <= 0 {
		return // Name and all subsequent fields are skipped.
	}
	// Maybe mutate Name
	if random.IntN(randRange) == 0 {
		s.SetName(pkg.StringRandom(random))
	}
	if fieldCount <= 1 {
		return // TimeUnixNano and all subsequent fields are skipped.
	}
	// Maybe mutate TimeUnixNano
	if random.IntN(randRange) == 0 {
		s.SetTimeUnixNano(pkg.Uint64Random(random))
	}
	if fieldCount <= 2 {
		return // Attributes and all subsequent fields are skipped.
	}
	// Maybe mutate Attributes
	if random.IntN(randRange) == 0 {
		s.attributes.mutateRandom(random, schem)
	}
	if fieldCount <= 3 {
		return // DroppedAttributesCount and all subsequent fields are skipped.
	}
	// Maybe mutate DroppedAttributesCount
	if random.IntN(randRange) == 0 {
		s.SetDroppedAttributesCount(pkg.Uint64Random(random))
	}
}

// IsEqual performs deep comparison and returns true if struct is equal to right.
func (s *Event) IsEqual(right *Event) bool {
	// Compare Name field.
	if !pkg.StringEqual(s.name, right.name) {
		return false
	}
	// Compare TimeUnixNano field.
	if !pkg.Uint64Equal(s.timeUnixNano, right.timeUnixNano) {
		return false
	}
	// Compare Attributes field.
	if !s.attributes.IsEqual(&right.attributes) {
		return false
	}
	// Compare DroppedAttributesCount field.
	if !pkg.Uint64Equal(s.droppedAttributesCount, right.droppedAttributesCount) {
		return false
	}

	return true
}

func EventEqual(left, right *Event) bool {
	return left.IsEqual(right)
}

// CmpEvent performs deep comparison and returns an integer that
// will be 0 if left == right, negative if left < right, positive if left > right.
func CmpEvent(left, right *Event) int {
	// Compare Name field.
	if c := strings.Compare(left.name, right.name); c != 0 {
		return c
	}
	// Compare TimeUnixNano field.
	if c := pkg.Uint64Compare(left.timeUnixNano, right.timeUnixNano); c != 0 {
		return c
	}
	// Compare Attributes field.
	if c := CmpAttributes(&left.attributes, &right.attributes); c != 0 {
		return c
	}
	// Compare DroppedAttributesCount field.
	if c := pkg.Uint64Compare(left.droppedAttributesCount, right.droppedAttributesCount); c != 0 {
		return c
	}
	return 0
}

// EventEncoder implements encoding of Event
type EventEncoder struct {
	buf     pkg.BitsWriter
	limiter *pkg.SizeLimiter

	// forceModifiedFields is set to a mask to force the next encoding operation
	// write the fields, whether they are modified or no. This is used after frame
	// restarts so that the data can be decoded from the frame start.
	forceModifiedFields uint64

	nameEncoder                   encoders.StringDictEncoder
	timeUnixNanoEncoder           encoders.Uint64Encoder
	attributesEncoder             *AttributesEncoder
	isAttributesRecursive         bool // Indicates Attributes field's type is recursive.
	droppedAttributesCountEncoder encoders.Uint64Encoder

	allocators *Allocators

	keepFieldMask uint64
	fieldCount    uint
}

func (e *EventEncoder) Init(state *WriterState, columns *pkg.WriteColumnSet) error {
	// Remember this encoder in the state so that we can detect recursion.
	if state.EventEncoder != nil {
		panic("cannot initialize EventEncoder: already initialized")
	}
	state.EventEncoder = e
	defer func() { state.EventEncoder = nil }()

	e.limiter = &state.limiter
	e.allocators = &state.Allocators

	// Number of fields in the output data schema.
	var err error
	e.fieldCount, err = state.StructFieldCounts.EventFieldCount()
	if err != nil {
		return fmt.Errorf("cannot find struct %s in override schema: %v", "Event", err)
	}
	// Set that many 1 bits in the keepFieldMask. All fields with higher number
	// will be skipped when encoding.
	e.keepFieldMask = ^(^uint64(0) << e.fieldCount)

	// Init encoder for Name field.
	if e.fieldCount <= 0 {
		return nil // Name and all subsequent fields are skipped.
	}
	err = e.nameEncoder.Init(&state.SpanEventName, e.limiter, columns.AddSubColumn())
	if err != nil {
		return err
	}

	// Init encoder for TimeUnixNano field.
	if e.fieldCount <= 1 {
		return nil // TimeUnixNano and all subsequent fields are skipped.
	}
	err = e.timeUnixNanoEncoder.Init(e.limiter, columns.AddSubColumn())
	if err != nil {
		return err
	}

	// Init encoder for Attributes field.
	if e.fieldCount <= 2 {
		return nil // Attributes and all subsequent fields are skipped.
	}
	if state.AttributesEncoder != nil {
		// Recursion detected, use the existing encoder.
		e.attributesEncoder = state.AttributesEncoder
		e.isAttributesRecursive = true
	} else {
		e.attributesEncoder = new(AttributesEncoder)
		err = e.attributesEncoder.Init(state, columns.AddSubColumn())
	}
	if err != nil {
		return err
	}

	// Init encoder for DroppedAttributesCount field.
	if e.fieldCount <= 3 {
		return nil // DroppedAttributesCount and all subsequent fields are skipped.
	}
	err = e.droppedAttributesCountEncoder.Init(e.limiter, columns.AddSubColumn())
	if err != nil {
		return err
	}

	return nil
}

func (e *EventEncoder) Reset() {
	// Since we are resetting the state of encoder make sure the next Encode()
	// call forcedly writes all fields and does not attempt to skip.
	e.forceModifiedFields = e.keepFieldMask

	if e.fieldCount <= 0 {
		return // Name and all subsequent fields are skipped.
	}
	e.nameEncoder.Reset()
	if e.fieldCount <= 1 {
		return // TimeUnixNano and all subsequent fields are skipped.
	}
	e.timeUnixNanoEncoder.Reset()
	if e.fieldCount <= 2 {
		return // Attributes and all subsequent fields are skipped.
	}
	if !e.isAttributesRecursive {
		e.attributesEncoder.Reset()
	}
	if e.fieldCount <= 3 {
		return // DroppedAttributesCount and all subsequent fields are skipped.
	}
	e.droppedAttributesCountEncoder.Reset()
}

// Encode encodes val into buf
func (e *EventEncoder) Encode(val *Event) {
	var bitCount uint

	// Mask that describes what fields are encoded. Start with all modified fields.
	fieldMask := val.modifiedFields.mask

	// If forceModifiedFields we need to set to 1 all bits so that we
	// force writing of all fields.
	fieldMask |= e.forceModifiedFields
	e.forceModifiedFields = 0

	// Only write fields that we want to write. See Init() for keepFieldMask.
	fieldMask &= e.keepFieldMask

	// Write bits to indicate which fields follow.
	e.buf.WriteBits(fieldMask, e.fieldCount)
	bitCount += e.fieldCount

	// Encode modified, present fields.

	if fieldMask&fieldModifiedEventName != 0 {
		// Encode Name
		e.nameEncoder.Encode(val.name)
	}

	if fieldMask&fieldModifiedEventTimeUnixNano != 0 {
		// Encode TimeUnixNano
		e.timeUnixNanoEncoder.Encode(val.timeUnixNano)
	}

	if fieldMask&fieldModifiedEventAttributes != 0 {
		// Encode Attributes
		e.attributesEncoder.Encode(&val.attributes)
	}

	if fieldMask&fieldModifiedEventDroppedAttributesCount != 0 {
		// Encode DroppedAttributesCount
		e.droppedAttributesCountEncoder.Encode(val.droppedAttributesCount)
	}

	// Account written bits in the limiter.
	e.limiter.AddFrameBits(bitCount)

	// Mark all fields non-modified so that next Encode() correctly
	// encodes only fields that change after this.
	val.modifiedFields.mask = 0
}

// CollectColumns collects all buffers from all encoders into buf.
func (e *EventEncoder) CollectColumns(columnSet *pkg.WriteColumnSet) {
	columnSet.SetBits(&e.buf)
	colIdx := 0
	// Collect Name field.
	if e.fieldCount <= 0 {
		return // Name and subsequent fields are skipped.
	}
	e.nameEncoder.CollectColumns(columnSet.At(colIdx))
	colIdx++
	// Collect TimeUnixNano field.
	if e.fieldCount <= 1 {
		return // TimeUnixNano and subsequent fields are skipped.
	}
	e.timeUnixNanoEncoder.CollectColumns(columnSet.At(colIdx))
	colIdx++
	// Collect Attributes field.
	if e.fieldCount <= 2 {
		return // Attributes and subsequent fields are skipped.
	}
	if !e.isAttributesRecursive {
		e.attributesEncoder.CollectColumns(columnSet.At(colIdx))
		colIdx++
	}
	// Collect DroppedAttributesCount field.
	if e.fieldCount <= 3 {
		return // DroppedAttributesCount and subsequent fields are skipped.
	}
	e.droppedAttributesCountEncoder.CollectColumns(columnSet.At(colIdx))
	colIdx++
}

// EventDecoder implements decoding of Event
type EventDecoder struct {
	buf         pkg.BitsReader
	column      *pkg.ReadableColumn
	fieldCount  uint
	nameDecoder encoders.StringDictDecoder

	timeUnixNanoDecoder encoders.Uint64Decoder

	attributesDecoder             *AttributesDecoder
	isAttributesRecursive         bool
	droppedAttributesCountDecoder encoders.Uint64Decoder

	allocators *Allocators
}

// Init is called once in the lifetime of the stream.
func (d *EventDecoder) Init(state *ReaderState, columns *pkg.ReadColumnSet) error {
	// Remember this decoder in the state so that we can detect recursion.
	if state.EventDecoder != nil {
		panic("cannot initialize EventDecoder: already initialized")
	}
	state.EventDecoder = d
	defer func() { state.EventDecoder = nil }()

	d.allocators = &state.Allocators

	// Number of fields in the input data schema.
	var err error
	d.fieldCount, err = state.StructFieldCounts.EventFieldCount()
	if err != nil {
		return fmt.Errorf("cannot find struct %s in override schema: %v", "Event", err)
	}

	d.column = columns.Column()

	if d.fieldCount <= 0 {
		return nil // Name and subsequent fields are skipped.
	}
	err = d.nameDecoder.Init(&state.SpanEventName, columns.AddSubColumn())
	if err != nil {
		return err
	}
	if d.fieldCount <= 1 {
		return nil // TimeUnixNano and subsequent fields are skipped.
	}
	err = d.timeUnixNanoDecoder.Init(columns.AddSubColumn())
	if err != nil {
		return err
	}
	if d.fieldCount <= 2 {
		return nil // Attributes and subsequent fields are skipped.
	}
	if state.AttributesDecoder != nil {
		// Recursion detected, use the existing decoder.
		d.attributesDecoder = state.AttributesDecoder
		d.isAttributesRecursive = true // Mark that we are using a recursive decoder.
	} else {
		d.attributesDecoder = new(AttributesDecoder)
		err = d.attributesDecoder.Init(state, columns.AddSubColumn())
	}
	if err != nil {
		return err
	}
	if d.fieldCount <= 3 {
		return nil // DroppedAttributesCount and subsequent fields are skipped.
	}
	err = d.droppedAttributesCountDecoder.Init(columns.AddSubColumn())
	if err != nil {
		return err
	}

	return nil
}

// Continue is called at the start of the frame to continue decoding column data.
// This should set the decoder's source buffer, so the new decoding continues from
// the supplied column data. This should NOT reset the internal state of the decoder,
// since columns can cross frame boundaries and the new column data is considered
// continuation of that same column in the previous frame.
func (d *EventDecoder) Continue() {
	d.buf.Reset(d.column.Data())

	if d.fieldCount <= 0 {
		return // Name and subsequent fields are skipped.
	}
	d.nameDecoder.Continue()
	if d.fieldCount <= 1 {
		return // TimeUnixNano and subsequent fields are skipped.
	}
	d.timeUnixNanoDecoder.Continue()
	if d.fieldCount <= 2 {
		return // Attributes and subsequent fields are skipped.
	}

	if !d.isAttributesRecursive {
		d.attributesDecoder.Continue()
	}

	if d.fieldCount <= 3 {
		return // DroppedAttributesCount and subsequent fields are skipped.
	}
	d.droppedAttributesCountDecoder.Continue()
}

func (d *EventDecoder) Reset() {

	if d.fieldCount <= 0 {
		return // Name and all subsequent fields are skipped.
	}
	d.nameDecoder.Reset()
	if d.fieldCount <= 1 {
		return // TimeUnixNano and all subsequent fields are skipped.
	}
	d.timeUnixNanoDecoder.Reset()
	if d.fieldCount <= 2 {
		return // Attributes and all subsequent fields are skipped.
	}

	if !d.isAttributesRecursive {
		d.attributesDecoder.Reset()
	}

	if d.fieldCount <= 3 {
		return // DroppedAttributesCount and all subsequent fields are skipped.
	}
	d.droppedAttributesCountDecoder.Reset()
}

func (d *EventDecoder) Decode(dstPtr *Event) error {
	val := dstPtr

	var err error

	// Read bits that indicate which fields follow.
	val.modifiedFields.mask = d.buf.ReadBits(d.fieldCount)

	if val.modifiedFields.mask&fieldModifiedEventName != 0 {
		// Field is changed and is present, decode it.
		err = d.nameDecoder.Decode(&val.name)
		if err != nil {
			return err
		}
	}

	if val.modifiedFields.mask&fieldModifiedEventTimeUnixNano != 0 {
		// Field is changed and is present, decode it.
		err = d.timeUnixNanoDecoder.Decode(&val.timeUnixNano)
		if err != nil {
			return err
		}
	}

	if val.modifiedFields.mask&fieldModifiedEventAttributes != 0 {
		// Field is changed and is present, decode it.
		err = d.attributesDecoder.Decode(&val.attributes)
		if err != nil {
			return err
		}
	}

	if val.modifiedFields.mask&fieldModifiedEventDroppedAttributesCount != 0 {
		// Field is changed and is present, decode it.
		err = d.droppedAttributesCountDecoder.Decode(&val.droppedAttributesCount)
		if err != nil {
			return err
		}
	}

	return nil
}

// EventAllocator implements a custom allocator for Event.
// It maintains a pool of pre-allocated Event and grows the pool
// dynamically as needed, up to a maximum size of 64 elements.
type EventAllocator struct {
	pool []Event
	ofs  int
}

// Alloc returns the next available Event from the pool.
// If the pool is exhausted, it grows the pool by doubling its size
// up to a maximum of 64 elements.
func (a *EventAllocator) Alloc() *Event {
	if a.ofs < len(a.pool) {
		// Get the next available Event from the pool
		a.ofs++
		return &a.pool[a.ofs-1]
	}
	// We've exhausted the current pool, prealloc a new pool.
	return a.prealloc()
}

//go:noinline
func (a *EventAllocator) prealloc() *Event {
	// prealloc expands the pool by doubling its size, up to a maximum of 64 elements.
	// If the pool is empty, it starts with 1 element.
	newLen := min(max(len(a.pool)*2, 1), 64)
	a.pool = make([]Event, newLen)
	a.ofs = 1
	return &a.pool[0]
}
